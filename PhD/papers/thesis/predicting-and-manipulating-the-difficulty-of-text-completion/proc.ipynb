{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "0b9b4d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "54798754",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples=[\n",
    "'Based on the analyses in this chapter, we conclude that text-\\ncompletion exercises are a reasonable choice for fast language proficiency tests.'\n",
    ",'They meet\\nthe educational quality criteria and can be generated and manipulated automatically be-\\ncause exercise content and exercise format are separable.'\n",
    ",'Half-open text-completion exer-\\ncises like C-tests and X-tests provide a trade-off between recognition and production exer-\\ncises and function as an integrative measure of linguistic knowledge and comprehension\\nabilities.'\n",
    "         ]\n",
    "examples = [ex.replace(\"-\\n\",\"[HSEP]\").replace(\"\\n\",\" \") for ex in examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "caf85224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Based on the analyses in this chapter, we conclude that text[HSEP]completion exercises are a reasonable choice for fast language proficiency tests.'\n",
      "'They meet the educational quality criteria and can be generated and manipulated automatically be[HSEP]cause exercise content and exercise format are separable.'\n",
      "'Half-open text-completion exer[HSEP]cises like C-tests and X-tests provide a trade-off between recognition and production exer[HSEP]cises and function as an integrative measure of linguistic knowledge and comprehension abilities.'\n"
     ]
    }
   ],
   "source": [
    "for e in examples:print(repr(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "460ce88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_unigram_vocab():\n",
    "\tvocab = {}\n",
    "\twith open(\"/Users/bernardostearns/scripts/unigram_freq.csv\") as inputf:\n",
    "\t\tfor line in inputf:\n",
    "\t\t\ttoken, freq = line.replace(\"\\n\",\"\").split(\",\")\n",
    "\t\t\tvocab[token] = freq\n",
    "\treturn vocab\t\n",
    "\n",
    "def get_sandwich_tokens(sent, sep_start_pos, sep_len):\n",
    "    '''\n",
    "         gets a space separated sentence as a string\n",
    "         and a position in the string.\n",
    "         returns the immediate (previous string and next string) to the position, \n",
    "         if the position is within a string it's not valid\n",
    "    \n",
    "        if i could do something like\n",
    "        \n",
    "        c1c2c3c4c5c6c7c8\n",
    "                            => (c1, idxc1, t1z) \n",
    "           t1   t2   t3\n",
    "        \n",
    "    '''\n",
    "    #### find t1\n",
    "    # go to pos\n",
    "    # set t1 end_pos to pos\n",
    "    end_t1_pos = sep_start_pos\n",
    "    if sent[end_t1_pos] == \" \":\n",
    "        return (\"token1\", None)\n",
    "    start_t1_pos = end_t1_pos\n",
    "    # while not find a space go backwards\n",
    "    while(sent[start_t1_pos] != \" \" and not(start_t1_pos == -1) ):\n",
    "        start_t1_pos -= 1\n",
    "    # when found set char right before space as start_pos\n",
    "    start_t1_pos += 1\n",
    "    t1 = sent[start_t1_pos:end_t1_pos]\n",
    "    \n",
    "    #### find t2\n",
    "    # go to pos\n",
    "    # set t2 start_pos to pos+len\n",
    "    start_t2_pos = sep_start_pos + sep_len\n",
    "    if sent[start_t2_pos] == \" \":\n",
    "        return (\"token2\", None)\n",
    "    # while not find a space go forward\n",
    "    end_t2_pos = start_t2_pos\n",
    "    while(sent[end_t2_pos] != \" \" and not(end_t2_pos == len(sent)) ):\n",
    "        end_t2_pos += 1\n",
    "    # when found set char right before space as start_pos\n",
    "    start_t1_pos -=1\n",
    "    t2 = sent[start_t2_pos:end_t2_pos]\n",
    "    return (t1,start_t1_pos,end_t1_pos),(t2,start_t2_pos,end_t2_pos)\n",
    "    \n",
    "def resolve(e):\n",
    "    print(e)\n",
    "    sep=\"[HSEP]\"\n",
    "    while(e.find(sep) != -1):\n",
    "        start_hsep = e.find(sep)\n",
    "        end_hsep = start_hsep + len(sep)\n",
    "        # immediate previous token before selected [HSEP]\n",
    "        (t1,s1,e1),(t2,s2,e2) = get_sandwich_tokens(e, start_hsep, len(sep))\n",
    "        candidates =[t1, '-'.join([t1,t2]), t2]\n",
    "        if('-'.join([t1,t2]) in vocab):\n",
    "            e = e[:s1+1] + '-'.join([t1,t2])  + e[e2:]\n",
    "        else:\n",
    "            e = e[:s1+1] + ''.join([t1,t2])  + e[e2:]\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "0a1bf646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "They meet the educational quality criteria and can be generated and manipulated automatically be[HSEP]cause exercise content and exercise format are separable.\n"
     ]
    }
   ],
   "source": [
    "resolve(examples[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "f03c174f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = load_unigram_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "99142498",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"be-cause\" in vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc0da80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
