@article{palenzuela2022modeling,
  title={Modeling Second Language Acquisition with pre-trained neural language models},
  author={Palenzuela, {\'A}lvaro J Jim{\'e}nez and Frasincar, Flavius and Tru{\c{s}}cǎ, Maria Mihaela},
  journal={Expert Systems with Applications},
  volume={207},
  pages={117871},
  year={2022},
  publisher={Elsevier},
  url="https://www.sciencedirect.com/science/article/abs/pii/S0957417422011241"
}

@inproceedings{renduchintala-etal-2016-user,
    title = "User Modeling in Language Learning with Macaronic Texts",
    author = "Renduchintala, Adithya  and
      Knowles, Rebecca  and
      Koehn, Philipp  and
      Eisner, Jason",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P16-1175",
    doi = "10.18653/v1/P16-1175",
    pages = "1859--1869",
}

@inproceedings{stahlberg-kumar-2021-synthetic,
    title = "Synthetic Data Generation for Grammatical Error Correction with Tagged Corruption Models",
    author = "Stahlberg, Felix  and
      Kumar, Shankar",
    booktitle = "Proceedings of the 16th Workshop on Innovative Use of NLP for Building Educational Applications",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.bea-1.4",
    pages = "37--47",
}

@inproceedings{lazar-etal-2021-filling,
    title = "Filling the Gaps in {A}ncient {A}kkadian Texts: A Masked Language Modelling Approach",
    author = "Lazar, Koren  and
      Saret, Benny  and
      Yehudai, Asaf  and
      Horowitz, Wayne  and
      Wasserman, Nathan  and
      Stanovsky, Gabriel",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.384",
    doi = "10.18653/v1/2021.emnlp-main.384",
    pages = "4682--4691",
}

@inproceedings{knowles2016analyzing,
  title={Analyzing learner understanding of novel L2 vocabulary},
  author={Knowles, Rebecca and Renduchintala, Adithya and Koehn, Philipp and Eisner, Jason},
  booktitle={Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning},
  pages={126--135},
  year={2016},
  url="https://aclanthology.org/K16-1013.pdf"
}

@inproceedings{avdiu-etal-2019-predicting,
    title = "Predicting learner knowledge of individual words using machine learning",
    author = "Avdiu, Drilon  and
      Bui, Vanessa  and
      Klim{\v{c}}{\'\i}kov{\'a}, Kl{\'a}ra Pta{\v{c}}inov{\'a}",
    booktitle = "Proceedings of the 8th Workshop on NLP for Computer Assisted Language Learning",
    month = sep,
    year = "2019",
    address = "Turku, Finland",
    publisher = "LiU Electronic Press",
    url = "https://aclanthology.org/W19-6301",
    pages = "1--9",
}

}


@inproceedings{zylich2021linguistic,
author = {Zylich, Brian and Lan, Andrew},
title = {Linguistic Skill Modeling for Second Language Acquisition},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448153},
doi = {10.1145/3448139.3448153},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {141–150},
numpages = {10},
keywords = {memory decay, student modeling, second language acquisition},
location = {Irvine, CA, USA},
series = {LAK21}
}



@article{portnoff2021methods,
  title={Methods for Language Learning Assessment at Scale: Duolingo Case Study.},
  author={Portnoff, Lucy and Gustafson, Erin and Rollinson, Joseph and Bicknell, Klinton},
  journal={International Educational Data Mining Society},
  year={2021},
  publisher={ERIC},
  url="https://eric.ed.gov/?id=ED615620"
}


@inproceedings{omelianchuk2020gector,
    title = "{GECT}o{R} {--} Grammatical Error Correction: Tag, Not Rewrite",
    author = "Omelianchuk, Kostiantyn  and
      Atrasevych, Vitaliy  and
      Chernodub, Artem  and
      Skurzhanskyi, Oleksandr",
    booktitle = "Proceedings of the Fifteenth Workshop on Innovative Use of NLP for Building Educational Applications",
    month = jul,
    year = "2020",
    address = "Seattle, WA, USA → Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.bea-1.16",
    doi = "10.18653/v1/2020.bea-1.16",
    pages = "163--170",
    abstract = "In this paper, we present a simple and efficient GEC sequence tagger using a Transformer encoder. Our system is pre-trained on synthetic data and then fine-tuned in two stages: first on errorful corpora, and second on a combination of errorful and error-free parallel corpora. We design custom token-level transformations to map input tokens to target corrections. Our best single-model/ensemble GEC tagger achieves an F{\_}0.5 of 65.3/66.5 on CONLL-2014 (test) and F{\_}0.5 of 72.4/73.6 on BEA-2019 (test). Its inference speed is up to 10 times as fast as a Transformer-based seq2seq GEC system.",
}

@inproceedings{geertzen2013automatic,
  title={Automatic linguistic annotation of large scale L2 databases: The EF-Cambridge Open Language Database (EFCAMDAT)},
  author={Geertzen, Jeroen and Alexopoulou, Theodora and Korhonen, Anna and others},
  booktitle={Proceedings of the 31st Second Language Research Forum. Somerville, MA: Cascadilla Proceedings Project},
  pages={240--254},
  year={2013},
  organization={Citeseer},
  url="https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=3261659127bebca4d805e8592906b37ece8d0ca3"
}


@inproceedings{bryant2017automatic,
    title = "Automatic Annotation and Evaluation of Error Types for Grammatical Error Correction",
    author = "Bryant, Christopher  and
      Felice, Mariano  and
      Briscoe, Ted",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P17-1074",
    doi = "10.18653/v1/P17-1074",
    pages = "793--805",
    abstract = "Until now, error type performance for Grammatical Error Correction (GEC) systems could only be measured in terms of recall because system output is not annotated. To overcome this problem, we introduce ERRANT, a grammatical ERRor ANnotation Toolkit designed to automatically extract edits from parallel original and corrected sentences and classify them according to a new, dataset-agnostic, rule-based framework. This not only facilitates error type evaluation at different levels of granularity, but can also be used to reduce annotator workload and standardise existing GEC datasets. Human experts rated the automatic edits as {``}Good{''} or {``}Acceptable{''} in at least 95{\%} of cases, so we applied ERRANT to the system output of the CoNLL-2014 shared task to carry out a detailed error type analysis for the first time.",
}


@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer.},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J and others},
  journal={J. Mach. Learn. Res.},
  volume={21},
  number={140},
  pages={1--67},
  year={2020},
 url="https://dl.acm.org/doi/abs/10.5555/3455716.3455856"
}



@inproceedings{settles-etal-2018-second,
    title = "Second Language Acquisition Modeling",
    author = "Settles, Burr  and
      Brust, Chris  and
      Gustafson, Erin  and
      Hagiwara, Masato  and
      Madnani, Nitin",
    booktitle = "Proceedings of the Thirteenth Workshop on Innovative Use of {NLP} for Building Educational Applications",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-0506",
    doi = "10.18653/v1/W18-0506",
    pages = "56--65",
    abstract = "We present the task of \textit{second language acquisition (SLA) modeling}. Given a history of errors made by learners of a second language, the task is to predict errors that they are likely to make at arbitrary points in the future. We describe a large corpus of more than 7M words produced by more than 6k learners of English, Spanish, and French using Duolingo, a popular online language-learning app. Then we report on the results of a shared task challenge aimed studying the SLA task via this corpus, which attracted 15 teams and synthesized work from various fields including cognitive science, linguistics, and machine learning.",
}


@inproceedings{settles2016trainable,
    title = "A Trainable Spaced Repetition Model for Language Learning",
    author = "Settles, Burr  and
      Meeder, Brendan",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P16-1174",
    doi = "10.18653/v1/P16-1174",
    pages = "1848--1858",
}

@inproceedings{devlin-etal-2019-bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}



@article{whitehill2017approximately,
  title={Approximately optimal teaching of approximately optimal learners},
  author={Whitehill, Jacob and Movellan, Javier},
  journal={IEEE Transactions on Learning Technologies},
  volume={11},
  number={2},
  pages={152--164},
  year={2017},
  publisher={IEEE},
  url="https://ieeexplore.ieee.org/abstract/document/7895197?casa_token=PJ5vhEzbRjIAAAAA:vPVBEcWHLHlKxQ_IFJu1vo2KvafGIG1WaD4jtofxu8mh_RAXYEqe-g3b40HirPlo8mdbJklmt40P"
}


@inproceedings{Miaschi2020LinguisticPO,
	  title={Linguistic Profiling of a Neural Language Model},
	  author={Alessio Miaschi and Dominique Brunato and Felice Dell’Orletta and Giulia Venturi},
	 booktitle={International Conference on Computational Linguistics},
	 year={2020},
      url="https://aclanthology.org/2020.coling-main.65/"
	}


@inproceedings{vulic-etal-2020-probing,
    title = "Probing Pretrained Language Models for Lexical Semantics",
    author = "Vuli{\'c}, Ivan  and
      Ponti, Edoardo Maria  and
      Litschko, Robert  and
      Glava{\v{s}}, Goran  and
      Korhonen, Anna",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.586",
    doi = "10.18653/v1/2020.emnlp-main.586",
    pages = "7222--7240",
    abstract = "The success of large pretrained language models (LMs) such as BERT and RoBERTa has sparked interest in probing their representations, in order to unveil what types of knowledge they implicitly capture. While prior research focused on morphosyntactic, semantic, and world knowledge, it remains unclear to which extent LMs also derive lexical type-level knowledge from words in context. In this work, we present a systematic empirical analysis across six typologically diverse languages and five different lexical tasks, addressing the following questions: 1) How do different lexical knowledge extraction strategies (monolingual versus multilingual source LM, out-of-context versus in-context encoding, inclusion of special tokens, and layer-wise averaging) impact performance? How consistent are the observed effects across tasks and languages? 2) Is lexical knowledge stored in few parameters, or is it scattered throughout the network? 3) How do these representations fare against traditional static word vectors in lexical tasks 4) Does the lexical information emerging from independently trained monolingual LMs display latent similarities? Our main results indicate patterns and best practices that hold universally, but also point to prominent variations across languages and tasks. Moreover, we validate the claim that lower Transformer layers carry more type-level lexical knowledge, but also show that this knowledge is distributed across multiple layers.",
}



@inproceedings{gururangan-etal-2020-dont,
    title = "Don{'}t Stop Pretraining: Adapt Language Models to Domains and Tasks",
    author = "Gururangan, Suchin  and
      Marasovi{\'c}, Ana  and
      Swayamdipta, Swabha  and
      Lo, Kyle  and
      Beltagy, Iz  and
      Downey, Doug  and
      Smith, Noah A.",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.740",
    doi = "10.18653/v1/2020.acl-main.740",
    pages = "8342--8360",
    abstract = "Language models pretrained on text from a wide variety of sources form the foundation of today{'}s NLP. In light of the success of these broad-coverage models, we investigate whether it is still helpful to tailor a pretrained model to the domain of a target task. We present a study across four domains (biomedical and computer science publications, news, and reviews) and eight classification tasks, showing that a second phase of pretraining in-domain (domain-adaptive pretraining) leads to performance gains, under both high- and low-resource settings. Moreover, adapting to the task{'}s unlabeled data (task-adaptive pretraining) improves performance even after domain-adaptive pretraining. Finally, we show that adapting to a task corpus augmented using simple data selection strategies is an effective alternative, especially when resources for domain-adaptive pretraining might be unavailable. Overall, we consistently find that multi-phase adaptive pretraining offers large gains in task performance.",
}

@inproceedings{sorokin-2022-improved,
    title = "Improved grammatical error correction by ranking elementary edits",
    author = "Sorokin, Alexey",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.785",
    pages = "11416--11429",
    abstract = "We offer a two-stage reranking method for grammatical error correction: the first model serves as edit generator, while the second classifies the proposed edits as correct or false. We show how to use both encoder-decoder and sequence labeling models for the first step of our pipeline. We achieve state-of-the-art quality on BEA 2019 English dataset even using weak BERT-GEC edit generator. Combining our roberta-base scorer with state-of-the-art GECToR edit generator, we surpass GECToR by 2-3{\%}. With a larger model we establish a new SOTA on BEA development and test sets. Our model also sets a new SOTA on Russian, despite using smaller models and less data than the previous approaches.",
}

@inproceedings{aoyama2022comparing,
  title={Comparing Native and Learner Englishes Using a Large Pre-trained Language Model},
  author={Aoyama, Tatsuya},
  booktitle={Proceedings of the 11th Workshop on NLP for Computer Assisted Language Learning},
  pages={1--9},
  year={2022},
  url="https://aclanthology.org/2022.nlp4call-1.1.pdf"
}