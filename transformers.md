[[deep_learning]]
	https://www.youtube.com/watch?v=f7TnuO02DjM
https://www.youtube.com/watch?v=wU8qg4JbJbk
https://www.youtube.com/watch?v=g2BRIuln4uc # Intuition Behind Self-Attention Mechanism in Transformer Networks


[Illustrated Guide to Transformers Neural Network: A step by step explanation](https://www.youtube.com/watch?v=4Bdc55j80l8)
[Visual Guide to Transformer Neural Networks - (Episode 1) Position Embeddings](https://www.youtube.com/watch?v=dichIcUZfOw)
[Rasa Algorithm Whiteboard - Transformers & Attention 1: Self Attention](https://www.youtube.com/watch?v=yGTUuEx3GkA)
[Attention Mechanism In a nutshell](https://www.youtube.com/watch?v=oMeIDqRguLY)
[Deep Learning 7. Attention and Memory in Deep Learning](https://www.youtube.com/watch?v=Q57rzaHHO0k)
[Lecture 12.1 Self-attention](https://www.youtube.com/watch?v=KmAISyVvE1Y)
[Transformers with Lucas Beyer, Google Brain](https://www.youtube.com/watch?v=EixI6t5oif0)
[Attention Is All You Need - Paper Explained](https://www.youtube.com/watch?v=XowwKOAWYoQ)
[Transfer learning and Transformer models (ML Tech Talks)](https://www.youtube.com/watch?v=LE3NfEULV6k)
[Vision Transformer for Image Classification](https://www.youtube.com/watch?v=HZ4j_U3FC94)
[What are TRANSFORMER Neural Networks?](https://www.youtube.com/watch?v=XSSTuhyAmnI)
[Vision Transformer in PyTorch](https://www.youtube.com/watch?v=ovB0ddFtzzA)
[Live -Transformers Indepth Architecture Understanding- Attention Is All You Need](https://www.youtube.com/watch?v=SMZQrJ_L1vo)
[225 - Attention U-net. What is attention and why is it needed for U-Net?](https://www.youtube.com/watch?v=KOF38xAvo8I)


[Ivan Bilan: Understanding and Applying Self-Attention for NLP | PyData Berlin 2018](https://www.youtube.com/watch?v=OYygPG4d9H0)
[Attention is all you need. A Transformer Tutorial. 1: Self-Attention](https://www.youtube.com/watch?v=1BFE1Tfs8tM)
[08L. Self-supervised learning and variational inference](https://www.youtube.com/watch?v=bdebHVF__mo)
[Linformer: Self-Attention with Linear Complexity (Paper Explained)](https://www.youtube.com/watch?v=-_2AF9Lhweo)
[Ali Ghodsi, Lect 10 (Fall 2020): Deep learning, Attention mechanism](https://www.youtube.com/watch?v=WFcH7kRNEBc)
[01L. Gradient descent and the backpropagation algorithm](https://www.youtube.com/watch?v=nTlCqaL7fCY)
[BERT Research - Ep. 5 - Inner Workings II - Self-Attention](https://www.youtube.com/watch?v=a1Hc9soLxts)
[Key Query Value Attention Explained](https://www.youtube.com/watch?V+H-4bmOxiKyU)
[09L. Differentiable associative memories, attention, and transformers](https://www.youtube.com/watch?v=AQtPoDnauq4)
[Rasa Algorithm Whiteboard - Transformers & Attention 2: Keys, Values, Queries](https://www.youtube.com/watch?v=tIvKXrEDMhk)
[The spelled-out intro to neural networks and backpropagation: building micrograd](https://www.youtube.com/watch?v=VMj-3S1tku0)
[Attention Is All You Need](https://www.youtube.com/watch?v=iDulhoQ2pro)
[Pytorch Transformers from Scratch (Attention is all you need)](https:??WWW>YOUTUBE>COM?WATCH?v=U0s0f995w14)
[Visual Guide to Transformer Neural Networks - (Episode 3) Decoders Masked Attention](https://www.youtube.com/watch?v=gJ9kaJsE78k)
[The Transformer neural network architecture EXPLAINED. Attention is all you need (NLP)](https://www.youtube.com/watch?v=FWFA4DGuzSc)
[Senior Software Engineer Mock Technical Interview (Coding/Algorithms in JavaScript)](https://www.youtube.com/watch?v=yju4zwKSriI)
[Attention is all you need; Attentional Neural Network Models | ukasz Kaiser | Masterclass](https://www.youtube.com/watch?v=rBCqOTEfxvg)
[https://www.youtube.com/watch?v=rBCqOTEfxvg](https://mail.google.com/mail/u/0/Â£inbox)
